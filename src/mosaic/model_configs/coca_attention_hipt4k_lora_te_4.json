{
    "embed_dim": 1024,
    "vision_cfg": {
        "input_dim": 192,
        "method": "vit",
        "pooling_strategy": "attentional",
        "attentional_pool_vector": true,
        "attn_pooler_vector_heads": 8,
        "attentional_pool_matrix": true,
        "attn_pooler_matrix_length": 256,
        "attn_pooler_matrix_heads": 8,
        "extra_config": {
            "attn_drop_rate": 0.5,
            "drop_rate": 0.0,
            "depth": 2,
            "num_heads": 4,
            "mlp_ratio":4,
            "qkv_bias": true
        }
    },
    "text_cfg": {
        "context_length": 76,
        "load_pretrained": true,
        "hf_model_name":"microsoft/biogpt",
        "hf_tokenizer_name":"microsoft/biogpt",
        "load_pretrained_model_start_layer": 0,
        "load_pretrained_model_end_layer": 12,
        "vocab_size": 42384,
        "heads": 16,
        "layers": 12,
        "freeze_base": true,
        "freeze_embedding": false,
        "attn_pooler_vector_heads": 8,
        "lora": true,
        "lora_config" : {
            "r": 4,
            "lora_alpha": 4,
            "target_modules": ["q_proj", "k_proj", "v_proj"],
            "lora_dropout": 0,
            "bias": "none"
        },
        "extra_config": {
            "attention_probs_dropout_prob": 0.5,
            "hidden_dropout_prob": 0.0,
            "activation_dropout": 0.0
        }
    },
    "multimodal_cfg": {
        "context_length": 76,
        "hf_model_name": "microsoft/biogpt",
        "load_pretrained_model_start_layer": 12,
        "load_pretrained_model_end_layer": 24,
        "vocab_size": 42384,
        "heads": 16,
        "layers": 12,
        "cross_attention_type": "default",
        "load_pretrained": true,
        "freeze_base": true,
        "freeze_cross_attn": false,
        "extra_config": {
            "attention_probs_dropout_prob": 0.0,
            "hidden_dropout_prob": 0.0,
            "activation_dropout": 0.0
        }
    },
    "custom_text": true

}